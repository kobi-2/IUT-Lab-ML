{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "logistic-regression-with-breast-cancer-data.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobi-2/IUT-Lab-ML/blob/master/logistic_regression_with_breast_cancer_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-0hGaBrF48Ue"
      },
      "source": [
        "# Reading the Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the Data\n",
        "data = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n",
        "data.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Sxoodi1rT34S"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UheCi8cf48Un"
      },
      "source": [
        "Let's have a look at the health of the data itself. This function `.info()` in the pandas library is very helpful to understand the basic properties of the data itself. If there is any missing values in the dataset can be known right from here so that they can be taken care of before fitting into a model for training and testing. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UL33cILW48Uo"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSh35FCx48Us"
      },
      "source": [
        "**There are 4 things that take my attention**\n",
        "\n",
        "1. There is an **`id`** that cannot be used for classificaiton \n",
        "2. **`Diagnosis`** column in the data is our class label\n",
        "3. **`Unnamed: 32`** feature includes **`NaN`** so we do not need it.\n",
        "4. I do not have any idea about other feature names actually.\n",
        "\n",
        "Therefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "r949QDzW48Us"
      },
      "source": [
        "# feature names as a list\n",
        "# .columns gives columns names in data \n",
        "col = data.columns       \n",
        "print(col)\n",
        "\n",
        "\n",
        "data.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\n",
        "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
        "y_data = data.diagnosis.values\n",
        "x_data = data.drop(['diagnosis'], axis=1)\n",
        "x_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pqJUEaNhT34b"
      },
      "source": [
        "y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmPQ3qi48Uw"
      },
      "source": [
        "Okey, now we have features but **`what does they mean`** or actually **`how much do we need to know about these features`**\n",
        "The answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values.\n",
        "These type of information helps to understand about what is going on data. For example , the question is appeared in my mind the **`area_mean`** feature's max value is 2500 and **`smoothness_mean`** features' max 0.16340. Therefore **do we need standirdization or normalization before visualization, feature selection, feature extraction or classificaiton?** The answer is yes and no not surprising ha :) Anyway lets go step by step and start with visualization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikmfvPcL48Ux"
      },
      "source": [
        "## Normalization\n",
        "Normalization refers to rescaling real-valued numeric attributes into a 0 to 1 range. Data normalization is used in machine learning to make model training less sensitive to the scale of features.\n",
        "\n",
        "You can either implement the conversion process with basic python or use `MinMaxScaler()` function from the `sklearn` library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BZfZTspj48Ux"
      },
      "source": [
        "# Using transformer from sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalar = MinMaxScaler()\n",
        "output = scalar.fit_transform(x_data)\n",
        "\n",
        "# Manual Implementation of the normalization process\n",
        "X_data = (x_data -np.min(x_data))/ (np.max(x_data)-np.min(x_data)).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KxRYAA46T34g"
      },
      "source": [
        "X_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faBBeeIw48U7"
      },
      "source": [
        "## The Basics: Logistic Regression and Regularization\n",
        "\n",
        "Logistic Regression is one of the most common machine learning algorithms used for classification. It a statistical model that uses a logistic function to model a binary dependent variable. In essence, it predicts the probability of an observation belonging to a certain class or label. For instance, is this a cat photo or a dog photo?\n",
        "\n",
        "Ordinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails. \n",
        "1. it is, expressly, a ‘regression’ framework, which makes it hard to apply as a classifier.\n",
        "2. unlike, say, a decision tree, linear regression models don’t perform their own implicit feature selection, meaning they are prone to overfit if too many features are included. \n",
        "\n",
        "Luckily, there are some extensions to the linear model that allow us to overcome these issues. Logistic regression turns the linear regression framework into a classifier and various types of **`regularization`** of which the `Ridge` and `Lasso` methods are most common, help avoid overfit in feature rich instances.\n",
        "\n",
        "### **Hypothesis:** \n",
        "We want our model to predict the probability of an observation belonging to a certain class or label. As such, we want a hypothesis $h$ that satisfies the following condition $0 <= h(x) <= 1$ , where $x$ is an observation.\n",
        "\n",
        "We define $h(x) = g(w^T * x)$ , where $g$ is a sigmoid function and $w$ are the trainable parameters or `weights`. As such, we have:\n",
        "$$h(x) = \\frac{1}{1+e^{-w^Tx}}$$\n",
        "\n",
        "### The cost for an observation: \n",
        "Now that we can predict the probability for an observation, we want the result to have the minimum error. If the class label is $y$, the cost (error) associated with an observation $x$ is given by:\n",
        "\n",
        "![](https://miro.medium.com/max/525/1*vSGnYVz6I7sAObKuxuFAoQ.gif)\n",
        "\n",
        "### Cost Function: \n",
        "Thus, the total cost for all the $m$ observations in a dataset is:\n",
        "![](https://miro.medium.com/max/368/0*vZnp94vCoN0vMDAj)\n",
        "\n",
        "We can rewrite the cost function J as:\n",
        "![](https://miro.medium.com/max/691/0*o57ug0iMGDJVI1qo)\n",
        "\n",
        "The objective of logistic regression is to find params `w` so that `J` is minimum. How can we do that?? We will use the gradient descent algorithm to update each of the weights gradually to minimize the cost `J`. \n",
        "\n",
        "We will update each of the params wᵢ using the following template:\n",
        "![](https://miro.medium.com/max/875/0*Q6ssvXABrvHUZrfy)\n",
        "![](https://miro.medium.com/max/496/0*7uVvuW-ZGauNWH_V)\n",
        "\n",
        "The above step will help us find a set of params wᵢ, which will then help us to come up with $h(x)$ to solve our binary classification task.\n",
        "But there is also an undesirable outcome associated with the above gradient descent steps. In an attempt to find the best $h(x)$, the following things happen:\n",
        "\n",
        "**CASE I: For class label = 0**: $h(x)$ will try to produce results as close 0 as possible. As such, $w^T.x$ will be as small as possible\n",
        "=> Wi will tend to -infinity\n",
        "\n",
        "**CASE II: For class label = 1**: $h(x)$ will try to produce results as close 1 as possible. As such, $w^T.x$ will be as large as possible\n",
        "=> Wi will tend to +infinity\n",
        "\n",
        "\n",
        "## Regularization:\n",
        "Regularization is a technique to solve the problem of overfitting in a machine learning algorithm by penalizing the cost function. It does so by using an additional penalty term in the cost function.\n",
        "There are two types of regularization techniques:\n",
        "1. Lasso or L1 Regularization\n",
        "2. Ridge or L2 Regularization (We will implement here)\n",
        "So, how can L2 Regularization help to prevent overfitting? Let’s first look at our new cost function:\n",
        "\n",
        "![](https://miro.medium.com/max/628/0*Nc_ocecF0dHpUutK)\n",
        "\n",
        "\n",
        "The regularization term will heavily penalize large $w_i$. The effect will be less on smaller $w_i$’s. As such, the growth of $w$ is controlled. The $h(x)$ we obtain with these controlled params $w$ will be more generalizable.\n",
        "\n",
        "**NOTE:** $λ$ is a hyper-parameter value. We have to find it using cross-validation. \n",
        "* Larger value $λ$ of will make $w_i$ shrink closer to $0$, which might lead to underfitting. \n",
        "* $λ = 0$, will have no regulariztion effect. \n",
        "\n",
        "When choosing $λ$, we have to take proper care of bias vs variance trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SIKhL69648U8"
      },
      "source": [
        "class LogisticRegression(object):\n",
        "    \"\"\"\n",
        "    Logistic Regression Classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : int or float, default=0.1\n",
        "        The tuning parameter for the optimization algorithm (here, Gradient Descent) \n",
        "        that determines the step size at each iteration while moving toward a minimum \n",
        "        of the cost function.\n",
        "    max_iter : int, default=100\n",
        "        Maximum number of iterations taken for the optimization algorithm to converge\n",
        "    \n",
        "    penalty : None or 'l2', default='l2'.\n",
        "        Option to perform L2 regularization.\n",
        "    C : float, default=0.1\n",
        "        Inverse of regularization strength; must be a positive float. \n",
        "        Smaller values specify stronger regularization. \n",
        "    tolerance : float, optional, default=1e-4\n",
        "        Value indicating the weight change between epochs in which\n",
        "        gradient descent should terminated. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, max_iter=100, regularization='l2', lambda_ = 10 , tolerance = 1e-4):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iter       = max_iter\n",
        "        self.regularization = regularization\n",
        "        self.lambda_        = lambda_\n",
        "        self.tolerance      = tolerance\n",
        "        self.loss_log       = []\n",
        "    \n",
        "    def fit(self, X, y, verbose = False):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            Training vector, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target vector relative to X.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        self.theta = np.random.rand(X.shape[1] + 1)\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        self.loss_log = []\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            Z = np.matmul(X,  self.theta)\n",
        "            y_hat = self.__sigmoid(Z)\n",
        "            \n",
        "            errors = y_hat - y\n",
        "\n",
        "            N = X.shape[1] \n",
        "            \n",
        "            cost = (-1.0/N) * np.sum( y*np.log(y_hat) + (1.0 - y)*np.log(1.0-y_hat))\n",
        "            self.loss_log.append(cost)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f'Iteration {iteration} Loss: {cost}')\n",
        "\n",
        "            if self.regularization is not None:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X)+ self.lambda_ * self.theta)\n",
        "            else:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X))\n",
        "                \n",
        "            self.theta -= self.learning_rate * delta_grad\n",
        "\n",
        "#             if np.all(abs(delta_grad) >= self.tolerance):\n",
        "#                 self.theta -= self.learning_rate * delta_grad\n",
        "#             else:\n",
        "#                 break\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Probability estimates for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Vector to be scored, where `n_samples` is the number of samples and\n",
        "            `n_features` is the number of features.\n",
        "        Returns\n",
        "        -------\n",
        "        probs : array-like of shape (n_samples,)\n",
        "            Returns the probability of each sample.\n",
        "        \"\"\"\n",
        "        return self.__sigmoid((X @ self.theta[1:]) + self.theta[0])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array_like or sparse matrix, shape (n_samples, n_features)\n",
        "            Samples.\n",
        "        Returns\n",
        "        -------\n",
        "        labels : array, shape [n_samples]\n",
        "            Predicted class label per sample.\n",
        "        \"\"\"\n",
        "        return np.round(self.predict_proba(X))\n",
        "        \n",
        "    def __sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        The sigmoid function.\n",
        "        Parameters\n",
        "        ------------\n",
        "        z : float\n",
        "            linear combinations of weights and sample features\n",
        "            z = w_0 + w_1*x_1 + ... + w_n*x_n\n",
        "        Returns\n",
        "        ---------\n",
        "        Value of logistic function at z\n",
        "        \"\"\"\n",
        "        return (1.0 / (1.0 + np.exp(-z)))\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Get method for models coeffients and intercept.\n",
        "        Returns\n",
        "        -------\n",
        "        params : dict\n",
        "        \"\"\"\n",
        "        try:\n",
        "            params = dict()\n",
        "            params['intercept'] = self.theta[0]\n",
        "            params['coef'] = self.theta[1:]\n",
        "            return params\n",
        "        except:\n",
        "            raise Exception('Fit the model first!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eseYei7-48U_"
      },
      "source": [
        "### Train and Test Data Validation\n",
        "Let us split the whole data into two portion. We take 80% data in the train set and then put rest of the data into the test set to check the performance of the trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w1HSZLIW48VA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n",
        "\n",
        "# Train and Test Data Summary\n",
        "import plotly.graph_objects as go\n",
        "split = ['Train','Test']\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=split, y=[np.sum(y_train), np.sum(y_test)],#                base=[-500,-600],\n",
        "                    marker_color='crimson',\n",
        "                    name='Malignant'))\n",
        "fig.add_trace(go.Bar(x=split, \n",
        "                     y=[len(y_train)- np.sum(y_train), len(y_test) - np.sum(y_test)],\n",
        "                    base=0,\n",
        "                    marker_color='lightgreen',\n",
        "                    name='Benign'                ))\n",
        "fig.update_layout(width = 800, height = 400)\n",
        "fig.update_layout(title = 'Count of Samples in Train and Test Split', title_x = 0.5, xaxis_title = \"Category\", yaxis_title = 'Sample Count')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQpWR7UrBv3L"
      },
      "source": [
        "## Training Logistic Regression\n",
        "Now we train the logistic regression with the training data for a maximum interation of 200. Other parameters are kept default. Feel free to fiddle around the other parameters to understand more of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q0egCbIh48VD"
      },
      "source": [
        "MAX_ITER = 400\n",
        "LR_RATE = 1e-2\n",
        "\n",
        "clf_no_reg = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE,  lambda_ = 200, regularization= None)\n",
        "clf_no_reg.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "clf_reg_10 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 10, regularization= 'l2')\n",
        "clf_reg_10.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "\n",
        "clf_reg_20 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 20, regularization= 'l2')\n",
        "clf_reg_20.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "clf_reg_40 = LogisticRegression(max_iter = MAX_ITER, learning_rate= LR_RATE, lambda_ = 40, regularization= 'l2')\n",
        "clf_reg_40.fit(X_train, y_train, verbose = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9u52S-aCJ-w"
      },
      "source": [
        "### Plot Training Loss over Time "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkK3DdF2CJfi",
        "trusted": true
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "y = clf_no_reg.loss_log \n",
        "\n",
        "# fig = go.Figure(data=go.Scatter(x= np.arange(start =1, stop = len(y)), \n",
        "#                                 y=y,\n",
        "#                                 mode = 'lines+markers'))\n",
        "\n",
        "\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_no_reg.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='No Regularization'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_10.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=10'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_20.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=20'))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=np.arange(start =1, stop = len(y)), y=clf_reg_40.loss_log ,\n",
        "                    mode='lines+markers',\n",
        "                    name='Regulrization W=40'))\n",
        "\n",
        "\n",
        "fig.update_layout(title = \"Error Plot over Iterations\", title_x = 0.5,\n",
        "                  xaxis_title = 'Iteration',\n",
        "                  yaxis_title = 'Log Loss',\n",
        "                  width = 800,\n",
        "                  height = 500)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50y75O6BDRAg"
      },
      "source": [
        "## Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vG00BWel48VH"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_no_reg.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_10.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_20.predict(X_test))*100}%')\n",
        "print(f'Accuracy {metrics.accuracy_score(y_test, clf_reg_40.predict(X_test))*100}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj3YQZaHEmlX"
      },
      "source": [
        "## Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EovXZHzEGyo",
        "trusted": true
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, clf_no_reg.predict(X_test)))\n",
        "sns.heatmap(confusion_matrix(y_test, clf_no_reg.predict(X_test)), annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QSOMbocaT34v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}